{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97555,"databundleVersionId":11670858,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:02:01.277058Z","iopub.execute_input":"2025-04-28T17:02:01.277617Z","iopub.status.idle":"2025-04-28T17:02:05.438150Z","shell.execute_reply.started":"2025-04-28T17:02:01.277596Z","shell.execute_reply":"2025-04-28T17:02:05.437573Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ==============================================\n# 1. Poisson-aware Data Loading & Preprocessing\n# ==============================================\n\nclass SPADDataset(Dataset):\n    def __init__(self, img_dir, depth_dir, frame_count=60, transform=None):\n        self.img_dir = img_dir\n        self.depth_dir = depth_dir\n        self.transform = transform\n        self.frame_count = frame_count\n        self.img_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n        \n    def __len__(self):\n        return len(self.img_files)\n    \n    def poisson_probability(self, binary_stack):\n        \"\"\"Convert binary frames to Poisson probability maps\"\"\"\n        p_observed = binary_stack.mean(axis=0, keepdims=True)  # Temporal mean\n        lambda_est = -torch.log(1 - p_observed + 1e-6)  # MLE for Poisson rate\n        return lambda_est\n    \n    def __getitem__(self, idx):\n        # Load binary frames (simulated SPAD data)\n        img_path = os.path.join(self.img_dir, self.img_files[idx])\n        binary_frames = torch.from_numpy(cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)).float() / 255.0\n        binary_frames = binary_frames.unsqueeze(0)  # (1, H, W)\n        \n        # Convert to Poisson probability map\n        input_img = self.poisson_probability(binary_frames)  # (1, H, W)\n        \n        # Load depth map\n        depth_path = os.path.join(self.depth_dir, self.img_files[idx])\n        depth_map = torch.from_numpy(cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)).float() / 255.0\n        depth_map = depth_map.unsqueeze(0)  # (1, H, W)\n        \n        if self.transform:\n            input_img = self.transform(input_img)\n            depth_map = self.transform(depth_map)\n            \n        return input_img, depth_map\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:02:25.828303Z","iopub.execute_input":"2025-04-28T17:02:25.829316Z","iopub.status.idle":"2025-04-28T17:02:25.836603Z","shell.execute_reply.started":"2025-04-28T17:02:25.829283Z","shell.execute_reply":"2025-04-28T17:02:25.835906Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ==============================================\n# 2. Attention-Enhanced U-Net Architecture\n# ==============================================\n\nclass CBAM(nn.Module):\n    \"\"\"Convolutional Block Attention Module\"\"\"\n    def __init__(self, channels, reduction=8):\n        super().__init__()\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, channels // reduction, 1),\n            nn.ReLU(),\n            nn.Conv2d(channels // reduction, channels, 1),\n            nn.Sigmoid()\n        )\n        self.spatial_attention = nn.Sequential(\n            nn.Conv2d(channels, 1, 7, padding=3),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        channel_att = self.channel_attention(x)\n        x_channel = x * channel_att\n        \n        spatial_att = self.spatial_attention(x_channel)\n        x_out = x_channel * spatial_att\n        return x_out\n\nclass UNetBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, use_attn=False):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU()\n        )\n        self.attn = CBAM(out_ch) if use_attn else nn.Identity()\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.attn(x)\n        return x\n\nclass SPAD_UNet(nn.Module):\n    def __init__(self, in_ch=1, out_ch=1, base_ch=32):\n        super().__init__()\n        # Encoder (Downsampling)\n        self.enc1 = UNetBlock(in_ch, base_ch, use_attn=True)\n        self.enc2 = UNetBlock(base_ch, base_ch*2)\n        self.enc3 = UNetBlock(base_ch*2, base_ch*4)\n        self.enc4 = UNetBlock(base_ch*4, base_ch*8)\n        self.pool = nn.MaxPool2d(2)\n        \n        # Bottleneck\n        self.bottleneck = UNetBlock(base_ch*8, base_ch*16)\n        \n        # Decoder (Upsampling)\n        self.upconv4 = nn.ConvTranspose2d(base_ch*16, base_ch*8, 2, stride=2)\n        self.dec4 = UNetBlock(base_ch*16, base_ch*8)\n        \n        self.upconv3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 2, stride=2)\n        self.dec3 = UNetBlock(base_ch*8, base_ch*4)\n        \n        self.upconv2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 2, stride=2)\n        self.dec2 = UNetBlock(base_ch*4, base_ch*2)\n        \n        self.upconv1 = nn.ConvTranspose2d(base_ch*2, base_ch, 2, stride=2)\n        self.dec1 = UNetBlock(base_ch*2, base_ch)\n        \n        # Output\n        self.outconv = nn.Conv2d(base_ch, out_ch, 1)\n        \n    def forward(self, x):\n        # Encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool(e1))\n        e3 = self.enc3(self.pool(e2))\n        e4 = self.enc4(self.pool(e3))\n        \n        # Bottleneck\n        bn = self.bottleneck(self.pool(e4))\n        \n        # Decoder with skip connections\n        d4 = self.upconv4(bn)\n        d4 = torch.cat((e4, d4), dim=1)\n        d4 = self.dec4(d4)\n        \n        d3 = self.upconv3(d4)\n        d3 = torch.cat((e3, d3), dim=1)\n        d3 = self.dec3(d3)\n        \n        d2 = self.upconv2(d3)\n        d2 = torch.cat((e2, d2), dim=1)\n        d2 = self.dec2(d2)\n        \n        d1 = self.upconv1(d2)\n        d1 = torch.cat((e1, d1), dim=1)\n        d1 = self.dec1(d1)\n        \n        # Output (linear activation for depth)\n        return self.outconv(d1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:02:42.360991Z","iopub.execute_input":"2025-04-28T17:02:42.361634Z","iopub.status.idle":"2025-04-28T17:02:42.373954Z","shell.execute_reply.started":"2025-04-28T17:02:42.361609Z","shell.execute_reply":"2025-04-28T17:02:42.373178Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ==============================================\n# 3. Hybrid Loss Function\n# ==============================================\n\nclass DepthLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ssim = SSIM(window_size=11)\n        \n    def forward(self, pred, target):\n        # RMSE\n        rmse = torch.sqrt(F.mse_loss(pred, target))\n        \n        # MAE\n        mae = F.l1_loss(pred, target)\n        \n        # SSIM (structural similarity)\n        ssim_loss = 1 - self.ssim(pred, target)\n        \n        return rmse + 0.5*mae + 0.1*ssim_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:02:57.662532Z","iopub.execute_input":"2025-04-28T17:02:57.662905Z","iopub.status.idle":"2025-04-28T17:02:57.667433Z","shell.execute_reply.started":"2025-04-28T17:02:57.662848Z","shell.execute_reply":"2025-04-28T17:02:57.666838Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ==============================================\n# 4. Training Pipeline\n# ==============================================\n\ndef train_model():\n    # Config\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    batch_size = 8\n    epochs = 50\n    lr = 1e-4\n    \n    # Data\n    train_dataset = SPADDataset(img_dir='train/spad', depth_dir='train/depth')\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    \n    # Model\n    model = SPAD_UNet().to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n    criterion = DepthLoss()\n    \n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0.0\n        \n        for inputs, targets in tqdm(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / len(train_loader)\n        scheduler.step(avg_loss)\n        \n        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n        \n        # Validation and model saving logic here\n        # ...\n\n    torch.save(model.state_dict(), 'spad_depth_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:03:17.751729Z","iopub.execute_input":"2025-04-28T17:03:17.752014Z","iopub.status.idle":"2025-04-28T17:03:17.758265Z","shell.execute_reply.started":"2025-04-28T17:03:17.751992Z","shell.execute_reply":"2025-04-28T17:03:17.757644Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ==============================================\n# 5. Inference & Submission\n# ==============================================\n\ndef predict_depth(model, img_path):\n    \"\"\"Generate depth map for a single SPAD image\"\"\"\n    model.eval()\n    with torch.no_grad():\n        # Load and preprocess image\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        img_tensor = torch.from_numpy(img).float().unsqueeze(0).unsqueeze(0) / 255.0\n        \n        # Convert to Poisson probability\n        lambda_est = -torch.log(1 - img_tensor.mean(dim=1, keepdim=True) + 1e-6)\n        \n        # Predict\n        depth = model(lambda_est.to(device))\n        depth = depth.squeeze().cpu().numpy()\n        \n    return depth\n\ndef create_submission(model, test_dir, output_csv):\n    \"\"\"Generate CSV for Kaggle submission\"\"\"\n    model.eval()\n    test_files = sorted([f for f in os.listdir(test_dir) if f.endswith('.png')])\n    \n    with open(output_csv, 'w') as f:\n        f.write('id,depth\\n')\n        \n        for img_file in tqdm(test_files):\n            img_path = os.path.join(test_dir, img_file)\n            depth_map = predict_depth(model, img_path)\n            \n            # Flatten and convert to CSV format\n            depth_flat = depth_map.flatten()\n            for i, val in enumerate(depth_flat):\n                f.write(f'{img_file[:-4]}_{i},{val:.6f}\\n')\n\n# Initialize and train\nif __name__ == '__main__':\n    train_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}