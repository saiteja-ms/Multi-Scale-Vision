{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97555,"databundleVersionId":11670858,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, callbacks\n# Import specific ResNet version if needed, depends on TensorFlow/Keras versions\n# from tensorflow.keras.applications.resnet import ResNet34 # Example if needed\n\nfrom tqdm.keras import TqdmCallback # Import TqdmCallback\nfrom tqdm import tqdm # Import standard tqdm for custom loops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:19:30.542110Z","iopub.execute_input":"2025-04-29T11:19:30.542677Z","iopub.status.idle":"2025-04-29T11:19:44.018736Z","shell.execute_reply.started":"2025-04-29T11:19:30.542652Z","shell.execute_reply":"2025-04-29T11:19:44.017840Z"}},"outputs":[{"name":"stderr","text":"2025-04-29 11:19:32.644375: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745925572.856597      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745925572.916111      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ======================\n# Configuration\n# ======================\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_CHANNELS = 1 # SPAD images are grayscale/single channel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:19:48.674927Z","iopub.execute_input":"2025-04-29T11:19:48.675721Z","iopub.status.idle":"2025-04-29T11:19:48.679276Z","shell.execute_reply.started":"2025-04-29T11:19:48.675694Z","shell.execute_reply":"2025-04-29T11:19:48.678455Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"TRAIN_SPAD_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-images' # Adjust folder names if needed\nTRAIN_GT_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-depths' # Adjust folder names if needed\nVAL_SPAD_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-images'   # Adjust folder names if needed\nVAL_GT_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-depths'   # Adjust folder names if needed\nTEST_SPAD_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images' # Adjust folder names if needed\nBASE_PREDICTION_OUTPUT_DIR = '/kaggle/working/Predictons'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:21:31.750064Z","iopub.execute_input":"2025-04-29T11:21:31.750885Z","iopub.status.idle":"2025-04-29T11:21:31.754534Z","shell.execute_reply.started":"2025-04-29T11:21:31.750854Z","shell.execute_reply":"2025-04-29T11:21:31.753761Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Training parameters\nBATCH_SIZE = 16\nEPOCHS = 50\nLEARNING_RATE = 3e-4 # Used with AdamW in the original snippet\nSEQ_LENGTH = 8 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:21:47.346091Z","iopub.execute_input":"2025-04-29T11:21:47.346796Z","iopub.status.idle":"2025-04-29T11:21:47.350246Z","shell.execute_reply.started":"2025-04-29T11:21:47.346771Z","shell.execute_reply":"2025-04-29T11:21:47.349425Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Prediction saving frequency\nSAVE_PREDICTIONS_FREQ = 5 # Save predictions every N epochs\n\n# For numerical stability with log\nEPSILON = 1e-7 # Use slightly larger epsilon for log(1-p)\n\n# ======================\n# Data Loading Helpers\n# ======================\n\ndef load_input_image(filepath):\n    \"\"\"\n    Loads the input image (assumed to be averaged binary or single frame result),\n    resizes, converts to [0, 1], and calculates phi_tau = -log(1 - integrated).\n    NOTE: This assumes the input filepath points directly to a single 8-bit PNG\n    which is the result of averaging 'seq_length' binary frames (implicitly).\n    \"\"\"\n    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n    if img is None:\n        # print(f\"Error loading image: {filepath}\") # Avoid spamming console\n        return None\n    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)\n\n    # Assuming img is already the result of averaging 'seq_length' binary frames\n    # Scale to [0, 1]\n    integrated = img.astype(np.float32) / 255.0\n\n    # Apply the Poisson correction to estimate rate * seq_length\n    # Clamp to avoid log(0) and log(<0)\n    integrated_clipped = np.clip(integrated, EPSILON, 1.0 - EPSILON)\n    phi_tau = -np.log(1.0 - integrated_clipped) # Ensure 1.0 is float\n\n    phi_tau = np.expand_dims(phi_tau, -1) # Add channel dimension\n    return phi_tau\n\ndef load_gt_depth(filepath):\n    \"\"\"\n    Loads a 16-bit single-channel ground truth depth image, resizes,\n    and normalizes to [0, 1].\n    \"\"\"\n    # Load as 16-bit grayscale (cv2.IMREAD_UNCHANGED or -1)\n    depth = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n    if depth is None:\n        # print(f\"Error loading depth: {filepath}\") # Avoid spamming console\n        return None\n    depth = cv2.resize(depth, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)\n\n    # Assuming 16-bit depth, normalize to [0, 1]\n    depth = depth.astype(np.float32) / 65535.0\n\n    depth = np.expand_dims(depth, -1) # Add channel dimension\n\n    return depth # Return normalized linear depth\n\ndef get_file_pairs(spad_dir, gt_dir):\n    \"\"\"Helper to list matching SPAD and GT depth files.\"\"\"\n    spad_files = sorted([f for f in os.listdir(spad_dir) if f.endswith('.png')])\n    gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith('.png')])\n    \n    # Match files based on filename (assuming they have the same base name)\n    spad_basenames = {os.path.splitext(f)[0]: f for f in spad_files}\n    gt_basenames = {os.path.splitext(f)[0]: f for f in gt_files}\n    \n    common_basenames = list(set(spad_basenames.keys()) & set(gt_basenames.keys()))\n    common_basenames.sort() # Ensure consistent order\n    \n    spad_paths = [os.path.join(spad_dir, spad_basenames[b]) for b in common_basenames]\n    gt_paths = [os.path.join(gt_dir, gt_basenames[b]) for b in common_basenames]\n\n    if len(spad_paths) != len(spad_files) or len(gt_paths) != len(gt_files):\n         print(f\"Warning: Found {len(spad_files)} SPAD files and {len(gt_files)} GT files in {spad_dir}/{gt_dir}, but only {len(spad_paths)} pairs matched filenames.\")\n\n    return spad_paths, gt_paths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:22:06.709838Z","iopub.execute_input":"2025-04-29T11:22:06.710105Z","iopub.status.idle":"2025-04-29T11:22:06.719991Z","shell.execute_reply.started":"2025-04-29T11:22:06.710083Z","shell.execute_reply":"2025-04-29T11:22:06.719332Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ======================\n# Data Pipeline (Generator)\n# ======================\nclass SPADGenerator(tf.keras.utils.Sequence):\n    def __init__(self, spad_paths, gt_paths, batch_size=BATCH_SIZE):\n        self.spad_paths = spad_paths\n        self.gt_paths = gt_paths\n        self.batch_size = batch_size\n        self.on_epoch_end()\n\n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch.\"\"\"\n        return len(self.spad_paths) // self.batch_size\n\n    def __getitem__(self, index):\n        \"\"\"Generate one batch of data.\"\"\"\n        # Get batch file paths\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        batch_spad_paths = [self.spad_paths[k] for k in indexes]\n        batch_gt_paths = [self.gt_paths[k] for k in indexes]\n\n        # Generate data\n        return self.__data_generation(batch_spad_paths, batch_gt_paths)\n\n    def on_epoch_end(self):\n        \"\"\"Updates indexes after each epoch.\"\"\"\n        self.indexes = np.arange(len(self.spad_paths))\n        np.random.shuffle(self.indexes)\n\n    def __data_generation(self, batch_spad_paths, batch_gt_paths):\n        \"\"\"Generates data containing batch_size samples.\"\"\"\n        X = np.empty((len(batch_spad_paths), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n        y = np.empty((len(batch_gt_paths), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n\n        loaded_count = 0\n        for i, (spad_path, gt_path) in enumerate(zip(batch_spad_paths, batch_gt_paths)):\n            phi_tau_img = load_input_image(spad_path)\n            gt_depth_norm = load_gt_depth(gt_path)\n\n            if phi_tau_img is None or gt_depth_norm is None:\n                 # Handle loading errors - ideally log this.\n                 # For simplicity, skip this sample. This might lead to a slightly smaller batch\n                 # if multiple samples fail in a batch. A robust generator handles this.\n                 continue # Skip this sample if loading fails\n\n            X[loaded_count,] = phi_tau_img\n            y[loaded_count,] = gt_depth_norm # This is normalized linear depth [0,1]\n            loaded_count += 1\n\n        # Return only the successfully loaded samples\n        if loaded_count < len(batch_spad_paths):\n             # print(f\"Warning: Batch size reduced from {len(batch_spad_paths)} to {loaded_count} due to loading errors.\")\n             pass # Avoid spamming console\n\n        return X[:loaded_count], y[:loaded_count]\n\n# ======================\n# Hybrid U-Net Architecture\n# ======================\ndef build_spadnet(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n    inputs = layers.Input(shape=input_shape)\n\n    # Use ResNet-34 if available, otherwise use a similar structure or simplify\n    # Note: ResNet34 is not a standard Keras application. ResNet50/101/152 are.\n    # If ResNet34 is specifically needed and not available, you'd need to implement it\n    # or find a custom implementation.\n    # Let's assume a ResNet model that takes 1 channel input and doesn't have the top layer.\n    # We'll use ResNet50 as an example, adapting it for 1 input channel.\n    # If your environment *does* have ResNet34 available via tf.keras.applications.resnet, uncomment that import.\n\n    # --- Coarse Path (Using adapted ResNet50 as an example) ---\n    # Note: Adapting a pre-trained ResNet for 1 channel input might require loading weights manually\n    # or training from scratch. Using weights=None here means training from scratch.\n    try:\n        # Attempt to use a standard ResNet (like 50) and adapt its input layer\n        base_resnet = tf.keras.applications.ResNet50(\n            include_top=False, weights=None, input_shape=input_shape\n        )\n        # The first conv layer in ResNet50 is typically 7x7, 64 filters, stride 2\n        # It expects 3 input channels. We need to replace or adapt it for 1 channel.\n        # A common trick is to load pre-trained weights (if weights='imagenet'),\n        # average the first conv kernel weights across input channels, and use that.\n        # Since weights=None, we just need a 1-channel input layer connected to the rest.\n\n        # Get output from the base ResNet before global pooling\n        # This gives the highest-level feature map from the ResNet body\n        coarse_backbone_output = base_resnet.output # Shape (?, H/32, W/32, 2048 for ResNet50)\n\n        # Apply Global Average Pooling\n        coarse_feat_global = layers.GlobalAveragePooling2D()(coarse_backbone_output) # Shape (?, 2048)\n\n        # Dense layers on the global feature (as in user's snippet)\n        coarse_feat_dense = layers.Dense(512, activation='relu')(coarse_feat_global) # Shape (?, 512)\n\n    except ImportError:\n        print(\"Warning: ResNet50 not found or could not be adapted. Building a simpler coarse path.\")\n        # Fallback: A few conv layers and then global pooling\n        x_coarse = layers.Conv2D(32, 5, strides=2, activation='relu', padding='same')(inputs)\n        x_coarse = layers.Conv2D(64, 3, strides=2, activation='relu', padding='same')(x_coarse)\n        x_coarse = layers.Conv2D(128, 3, strides=2, activation='relu', padding='same')(x_coarse)\n        coarse_feat_global = layers.GlobalAveragePooling2D()(x_coarse)\n        coarse_feat_dense = layers.Dense(512, activation='relu')(coarse_feat_global)\n\n    # --- Fine Path (U-Net) ---\n    # Standard U-Net encoder\n    conv1 = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n    conv1 = layers.Conv2D(32, 3, activation='relu', padding='same')(conv1) # Shape (?, 256, 256, 32)\n    pool1 = layers.MaxPooling2D()(conv1) # Shape (?, 128, 128, 32)\n\n    conv2 = layers.Conv2D(64, 3, activation='relu', padding='same')(pool1)\n    conv2 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv2) # Shape (?, 128, 128, 64)\n    pool2 = layers.MaxPooling2D()(conv2) # Shape (?, 64, 64, 64)\n\n    conv3 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool2)\n    conv3 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv3) # Shape (?, 64, 64, 128)\n    pool3 = layers.MaxPooling2D()(conv3) # Shape (?, 32, 32, 128)\n\n    conv4 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool3)\n    conv4 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv4) # Shape (?, 32, 32, 256)\n    # No dropout here in the user's snippet, but adding it can help prevent overfitting\n    # drop4 = layers.Dropout(0.5)(conv4)\n\n    # Bottleneck\n    bottleneck = layers.MaxPooling2D()(conv4) # Shape (?, 16, 16, 256)\n    bottleneck = layers.Conv2D(512, 3, activation='relu', padding='same')(bottleneck) # Shape (?, 16, 16, 512)\n    bottleneck = layers.Conv2D(512, 3, activation='relu', padding='same')(bottleneck) # Shape (?, 16, 16, 512) # Added extra conv for symmetry/capacity\n\n\n    # --- Fusion ---\n    # Fuse the global coarse_feat_dense with the spatial bottleneck features\n    # coarse_feat_dense shape: (?, 512)\n    # bottleneck shape: (?, 16, 16, 512)\n\n    # Reshape global feature to (batch_size, 1, 1, 512)\n    coarse_feat_reshaped = layers.Reshape((1, 1, 512))(coarse_feat_dense)\n\n    # Spatially replicate the global feature to match bottleneck spatial dimensions\n    # Get dynamic H, W from bottleneck\n    bottleneck_spatial_shape = tf.shape(bottleneck)[1:3]\n    coarse_feat_replicated = tf.tile(coarse_feat_reshaped, [1, bottleneck_spatial_shape[0], bottleneck_spatial_shape[1], 1])\n\n    # Concatenate replicated global feature with bottleneck feature\n    # Results in shape (?, 16, 16, 512 + 512 = 1024)\n    fused = layers.Concatenate()([bottleneck, coarse_feat_replicated])\n\n\n    # --- Decoder ---\n    # Upsample and concatenate with skip connections\n    up6 = layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(fused) # Shape (?, 32, 32, 256)\n    merge6 = layers.concatenate([conv4, up6], axis=3) # Skip connection from conv4 (shape ?, 32, 32, 256). Result shape (?, 32, 32, 512)\n    conv6 = layers.Conv2D(256, 3, activation='relu', padding='same')(merge6)\n    conv6 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv6) # Shape (?, 32, 32, 256)\n\n    up7 = layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv6) # Shape (?, 64, 64, 128)\n    merge7 = layers.concatenate([conv3, up7], axis=3) # Skip connection from conv3 (shape ?, 64, 64, 128). Result shape (?, 64, 64, 256)\n    conv7 = layers.Conv2D(128, 3, activation='relu', padding='same')(merge7)\n    conv7 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv7) # Shape (?, 64, 64, 128)\n\n    up8 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv7) # Shape (?, 128, 128, 64)\n    merge8 = layers.concatenate([conv2, up8], axis=3) # Skip connection from conv2 (shape ?, 128, 128, 64). Result shape (?, 128, 128, 128)\n    conv8 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge8)\n    conv8 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv8) # Shape (?, 128, 128, 64)\n\n    up9 = layers.Conv2DTranspose(32, 2, strides=(2, 2), padding='same')(conv8) # Shape (?, 256, 256, 32)\n    merge9 = layers.concatenate([conv1, up9], axis=3) # Skip connection from conv1 (shape ?, 256, 256, 32). Result shape (?, 256, 256, 64)\n    conv9 = layers.Conv2D(32, 3, activation='relu', padding='same')(merge9)\n    conv9 = layers.Conv2D(32, 3, activation='relu', padding='same')(conv9) # Shape (?, 256, 256, 32)\n\n    # Output layer predicts normalized linear depth [0,1]\n    outputs = layers.Conv2D(1, 1, activation='sigmoid', dtype='float32')(conv9) # Ensure float32 output\n\n    return Model(inputs=inputs, outputs=outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:22:41.068516Z","iopub.execute_input":"2025-04-29T11:22:41.069228Z","iopub.status.idle":"2025-04-29T11:22:41.295369Z","shell.execute_reply.started":"2025-04-29T11:22:41.069200Z","shell.execute_reply":"2025-04-29T11:22:41.294451Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ======================\n# Prediction Saving Function (Loop-based for simplicity)\n# ======================\ndef save_predictions_for_test_set_loop(model, test_spad_paths, output_dir):\n    \"\"\"Predicts on test set and saves scaled depth PNGs using a loop.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    for i, spad_filepath in enumerate(tqdm(test_spad_paths, desc=f\"Saving to {os.path.basename(output_dir)}\")):\n        phi_tau_img = load_input_image(spad_filepath)\n        if phi_tau_img is None:\n            # Error already printed in load_input_image\n            continue\n\n        img_batch = np.expand_dims(phi_tau_img, axis=0) # Add batch dimension\n\n        # Predict (model outputs normalized linear depth [0,1])\n        predicted_depth_norm = model.predict(img_batch, verbose=0)[0] # Remove batch dimension\n\n        # --- Scaling for PNG Save (0-255) ---\n        # The model output is [0,1] linear depth.\n        # imgs2csv.py expects 0-255 uint8 PNGs, which it will then MIN-MAX scale to [0,1]\n        # Saving the direct [0,1] output scaled to [0,255] is standard for this.\n        scaled_uint8_depth = np.clip(predicted_depth_norm * 255.0, 0, 255).astype(np.uint8)\n\n        # Remove the channel dimension for cv2.imwrite\n        scaled_uint8_depth = np.squeeze(scaled_uint8_depth)\n\n        # Save the prediction with the original filename\n        output_filename = os.path.basename(spad_filepath)\n        output_filepath = os.path.join(output_dir, output_filename)\n        cv2.imwrite(output_filepath, scaled_uint8_depth)\n\n\n# ======================\n# Custom Callback for Periodic Saving\n# ======================\nclass PeriodicPredictionSaverCallback(callbacks.Callback):\n    def __init__(self, test_spad_paths, save_freq, base_output_dir):\n        super().__init__()\n        self.test_spad_paths = test_spad_paths\n        self.save_freq = save_freq\n        self.base_output_dir = base_output_dir\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Save predictions every self.save_freq epochs (starting from epoch 0, so +1 for display)\n        if (epoch + 1) % self.save_freq == 0:\n            print(f\"\\nSaving predictions after epoch {epoch + 1}...\")\n            epoch_output_dir = os.path.join(self.base_output_dir, f\"epoch_{epoch+1:03d}\")\n            save_predictions_for_test_set_loop( # Use the loop-based saver\n                self.model, # The model instance is accessible via self.model\n                self.test_spad_paths,\n                epoch_output_dir\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:22:59.804348Z","iopub.execute_input":"2025-04-29T11:22:59.804792Z","iopub.status.idle":"2025-04-29T11:22:59.811589Z","shell.execute_reply.started":"2025-04-29T11:22:59.804770Z","shell.execute_reply":"2025-04-29T11:22:59.810829Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ======================\n# Training Configuration\n# ======================\ndef main():\n    # --- Prepare Data File Lists ---\n    print(\"Preparing data file lists...\")\n\n    # Get file lists for training and validation\n    train_spad_paths, train_gt_paths = get_file_pairs(TRAIN_SPAD_DIR, TRAIN_GT_DIR)\n    val_spad_paths, val_gt_paths = get_file_pairs(VAL_SPAD_DIR, VAL_GT_DIR)\n\n    # Test data only needs input SPAD paths\n    test_spad_paths = sorted([os.path.join(TEST_SPAD_DIR, f) for f in os.listdir(TEST_SPAD_DIR) if f.endswith('.png')])\n\n    print(f\"Training samples: {len(train_spad_paths)}\")\n    print(f\"Validation samples: {len(val_spad_paths)}\")\n    print(f\"Test samples: {len(test_spad_paths)}\")\n\n    if not train_spad_paths or not val_spad_paths or not test_spad_paths:\n        print(\"\\nError: One or more data directories are empty or paths are incorrect!\")\n        print(\"Please check DATA_DIR and subfolder names (TRAIN_SPAD_DIR, etc.)\")\n        return # Exit if data is not found\n\n    # Data generators\n    train_gen = SPADGenerator(train_spad_paths, train_gt_paths, batch_size=BATCH_SIZE)\n    val_gen = SPADGenerator(val_spad_paths, val_gt_paths, batch_size=BATCH_SIZE)\n\n    # Model setup\n    print(\"Building model...\")\n    model = build_spadnet(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n\n    print(\"Compiling model...\")\n    # Compile using user's specified loss and metric\n    model.compile(\n        optimizer=tf.keras.optimizers.AdamW(LEARNING_RATE),\n        loss='mse', # Mean Squared Error on normalized [0,1] depth\n        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')] # Use 'rmse' for monitoring\n    )\n\n    # Optional: Print model summary\n    # model.summary()\n\n    # --- Set up Callbacks ---\n    print(\"Setting up callbacks...\")\n    # Checkpoint needs .keras extension in modern Keras\n    checkpoint_filepath = 'best_model.keras'\n\n    callbacks_list = [\n        callbacks.ModelCheckpoint(\n            checkpoint_filepath,\n            save_best_only=True,\n            monitor='val_rmse', # Monitor validation RMSE\n            mode='min',\n            verbose=0 # Keep checkpoint callback quiet\n        ),\n        PeriodicPredictionSaverCallback(\n             test_spad_paths=test_spad_paths,\n             save_freq=SAVE_PREDICTIONS_FREQ, # Save every N epochs\n             base_output_dir=BASE_PREDICTION_OUTPUT_DIR,\n        ),\n        callbacks.ReduceLROnPlateau(\n            monitor='val_rmse', # Monitor validation RMSE\n            factor=0.5, # Reduce learning rate by half\n            patience=3, # If val_rmse doesn't improve for 3 epochs\n            mode='min',\n            min_lr=1e-6 # Minimum learning rate\n        ),\n        callbacks.EarlyStopping(\n            monitor='val_rmse',\n            patience=10, # Stop if val_rmse doesn't improve for 10 epochs\n            mode='min',\n            restore_best_weights=True # Use weights from the best epoch\n        ),\n        TqdmCallback(verbose=1) # Progress bar callback\n    ]\n\n    # --- Training ---\n    print(\"Starting training...\")\n    # Removed workers/use_multiprocessing to avoid TypeError\n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=EPOCHS,\n        callbacks=callbacks_list,\n        verbose=0 # Let TqdmCallback handle verbosity\n    )\n\n    print(\"Training finished.\")\n\n    # --- Final Prediction and Saving ---\n    # Load the best model weights saved by ModelCheckpoint\n    # Note: EarlyStopping with restore_best_weights=True often makes this redundant,\n    # but explicitly loading ensures you use the best saved model state, especially\n    # if training finished without early stopping.\n    if os.path.exists(checkpoint_filepath):\n        print(f\"Loading best model weights from {checkpoint_filepath}\")\n        model.load_weights(checkpoint_filepath)\n    else:\n        print(\"No checkpoint found. Using model state after last epoch.\")\n\n    print(\"\\nGenerating final predictions with the best model...\")\n    final_output_dir = os.path.join(BASE_PREDICTION_OUTPUT_DIR, \"final_best_model\")\n    save_predictions_for_test_set_loop( # Use the loop-based saver\n        model,\n        test_spad_paths,\n        final_output_dir\n    )\n    print(f\"Final predictions saved to {final_output_dir}\")\n\n\n    # --- Submission Instructions ---\n    print(\"\\n--- Submission Step ---\")\n    print(\"1. Ensure you have the imgs2csv.py script provided by the challenge.\")\n    print(f\"2. The predicted depth maps are saved in subfolders of: {BASE_PREDICTION_OUTPUT_DIR}\")\n    print(\"3. Choose the folder you want to submit (e.g., 'final_best_model' for the best model, or 'epoch_XXX' for periodic saves).\")\n    print(\"4. Open a terminal/notebook cell and run the conversion script, pointing to the chosen folder:\")\n    # Adjust the path to where your imgs2csv.py is located on Kaggle\n    print(f\"   python /kaggle/input/spad-depth-challenge/imgs2csv.py {final_output_dir} submission.csv\") # **ADJUST THIS PATH**\n    print(\"5. Submit the generated 'submission.csv' file to Kaggle.\")\n    print(\"\\nNote: Replace '/kaggle/input/spad-depth-challenge/imgs2csv.py' with the actual path to the imgs2csv.py script on Kaggle.\")\n    print(\"----------------------\")\n\n\nif __name__ == \"__main__\":\n    # Set default float type for consistency\n    tf.keras.backend.set_floatx('float32')\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:23:51.380939Z","iopub.execute_input":"2025-04-29T11:23:51.381469Z","iopub.status.idle":"2025-04-29T11:23:54.800756Z","shell.execute_reply.started":"2025-04-29T11:23:51.381449Z","shell.execute_reply":"2025-04-29T11:23:54.799756Z"}},"outputs":[{"name":"stdout","text":"Preparing data file lists...\nTraining samples: 6686\nValidation samples: 836\nTest samples: 836\nBuilding model...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745925832.644277      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1523020439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Set default float type for consistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_floatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/1523020439.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Model setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_spadnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_CHANNELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Compiling model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2316686853.py\u001b[0m in \u001b[0;36mbuild_spadnet\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# Spatially replicate the global feature to match bottleneck spatial dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Get dynamic H, W from bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mbottleneck_spatial_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mcoarse_feat_replicated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoarse_feat_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck_spatial_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck_spatial_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/keras_tensor.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__tf_tensor__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;34m\"A KerasTensor cannot be used as input to a TensorFlow function. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;34m\"A KerasTensor is a symbolic placeholder for a shape and dtype, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"],"ename":"ValueError","evalue":"A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, callbacks\nfrom tqdm.keras import TqdmCallback\nfrom tqdm import tqdm\n\n# ======================\n# Configuration\n# ======================\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_CHANNELS = 1 # SPAD images are grayscale/single channel\n\n# Kaggle input directory structure\nTRAIN_SPAD_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-images'\nTRAIN_GT_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-depths'\nVAL_SPAD_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-images'\nVAL_GT_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-depths'\nTEST_SPAD_DIR = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images'\nBASE_PREDICTION_OUTPUT_DIR = '/kaggle/working/predictions_png'\n\nBATCH_SIZE = 16\nEPOCHS = 50\nLEARNING_RATE = 3e-4\nSEQ_LENGTH = 8\nSAVE_PREDICTIONS_FREQ = 5\nEPSILON = 1e-7\n\n# ======================\n# Data Loading Helpers\n# ======================\n\ndef load_input_image(filepath):\n    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n    if img is None:\n        return None\n    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)\n    integrated = img.astype(np.float32) / 255.0\n    integrated_clipped = np.clip(integrated, EPSILON, 1.0 - EPSILON)\n    phi_tau = -np.log(1.0 - integrated_clipped)\n    phi_tau = np.expand_dims(phi_tau, -1)\n    return phi_tau\n\ndef load_gt_depth(filepath):\n    depth = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n    if depth is None:\n        return None\n    depth = cv2.resize(depth, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)\n    depth = depth.astype(np.float32) / 65535.0\n    depth = np.expand_dims(depth, -1)\n    return depth\n\ndef get_file_pairs(spad_dir, gt_dir):\n    spad_files = sorted([f for f in os.listdir(spad_dir) if f.endswith('.png')])\n    gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith('.png')])\n    spad_basenames = {os.path.splitext(f)[0]: f for f in spad_files}\n    gt_basenames = {os.path.splitext(f)[0]: f for f in gt_files}\n    common_basenames = list(set(spad_basenames.keys()) & set(gt_basenames.keys()))\n    common_basenames.sort()\n    spad_paths = [os.path.join(spad_dir, spad_basenames[b]) for b in common_basenames]\n    gt_paths = [os.path.join(gt_dir, gt_basenames[b]) for b in common_basenames]\n    if len(spad_paths) != len(spad_files) or len(gt_paths) != len(gt_files):\n        print(f\"Warning: Found {len(spad_files)} SPAD files and {len(gt_files)} GT files in {spad_dir}/{gt_dir}, but only {len(spad_paths)} pairs matched filenames.\")\n    return spad_paths, gt_paths\n\n# ======================\n# Data Pipeline (Generator)\n# ======================\nclass SPADGenerator(tf.keras.utils.Sequence):\n    def __init__(self, spad_paths, gt_paths, batch_size=BATCH_SIZE):\n        self.spad_paths = spad_paths\n        self.gt_paths = gt_paths\n        self.batch_size = batch_size\n        self.on_epoch_end()\n\n    def __len__(self):\n        return len(self.spad_paths) // self.batch_size\n\n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        batch_spad_paths = [self.spad_paths[k] for k in indexes]\n        batch_gt_paths = [self.gt_paths[k] for k in indexes]\n        return self.__data_generation(batch_spad_paths, batch_gt_paths)\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.spad_paths))\n        np.random.shuffle(self.indexes)\n\n    def __data_generation(self, batch_spad_paths, batch_gt_paths):\n        X = np.empty((len(batch_spad_paths), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n        y = np.empty((len(batch_gt_paths), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n        loaded_count = 0\n        for i, (spad_path, gt_path) in enumerate(zip(batch_spad_paths, batch_gt_paths)):\n            phi_tau_img = load_input_image(spad_path)\n            gt_depth_norm = load_gt_depth(gt_path)\n            if phi_tau_img is None or gt_depth_norm is None:\n                continue\n            X[loaded_count,] = phi_tau_img\n            y[loaded_count,] = gt_depth_norm\n            loaded_count += 1\n        return X[:loaded_count], y[:loaded_count]\n\n# ======================\n# Hybrid U-Net Architecture\n# ======================\ndef build_spadnet(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n    inputs = layers.Input(shape=input_shape)\n    # --- Fine Path (U-Net) ---\n    conv1 = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n    conv1 = layers.Conv2D(32, 3, activation='relu', padding='same')(conv1)\n    pool1 = layers.MaxPooling2D()(conv1)\n    conv2 = layers.Conv2D(64, 3, activation='relu', padding='same')(pool1)\n    conv2 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv2)\n    pool2 = layers.MaxPooling2D()(conv2)\n    conv3 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool2)\n    conv3 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv3)\n    pool3 = layers.MaxPooling2D()(conv3)\n    conv4 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool3)\n    conv4 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv4)\n    bottleneck = layers.MaxPooling2D()(conv4)\n    bottleneck = layers.Conv2D(512, 3, activation='relu', padding='same')(bottleneck)\n    bottleneck = layers.Conv2D(512, 3, activation='relu', padding='same')(bottleneck)\n    # Decoder\n    up6 = layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(bottleneck)\n    merge6 = layers.concatenate([conv4, up6], axis=3)\n    conv6 = layers.Conv2D(256, 3, activation='relu', padding='same')(merge6)\n    conv6 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv6)\n    up7 = layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv6)\n    merge7 = layers.concatenate([conv3, up7], axis=3)\n    conv7 = layers.Conv2D(128, 3, activation='relu', padding='same')(merge7)\n    conv7 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv7)\n    up8 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv7)\n    merge8 = layers.concatenate([conv2, up8], axis=3)\n    conv8 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge8)\n    conv8 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv8)\n    up9 = layers.Conv2DTranspose(32, 2, strides=(2, 2), padding='same')(conv8)\n    merge9 = layers.concatenate([conv1, up9], axis=3)\n    conv9 = layers.Conv2D(32, 3, activation='relu', padding='same')(merge9)\n    conv9 = layers.Conv2D(32, 3, activation='relu', padding='same')(conv9)\n    outputs = layers.Conv2D(1, 1, activation='sigmoid', dtype='float32')(conv9)\n    return Model(inputs=inputs, outputs=outputs)\n\n# ======================\n# Prediction Saving Function (Loop-based for simplicity)\n# ======================\ndef save_predictions_for_test_set_loop(model, test_spad_paths, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    for i, spad_filepath in enumerate(tqdm(test_spad_paths, desc=f\"Saving to {os.path.basename(output_dir)}\")):\n        phi_tau_img = load_input_image(spad_filepath)\n        if phi_tau_img is None:\n            continue\n        img_batch = np.expand_dims(phi_tau_img, axis=0)\n        predicted_depth_norm = model.predict(img_batch, verbose=0)[0]\n        scaled_uint8_depth = np.clip(predicted_depth_norm * 255.0, 0, 255).astype(np.uint8)\n        scaled_uint8_depth = np.squeeze(scaled_uint8_depth)\n        output_filename = os.path.basename(spad_filepath)\n        output_filepath = os.path.join(output_dir, output_filename)\n        cv2.imwrite(output_filepath, scaled_uint8_depth)\n\n# ======================\n# Custom Callback for Periodic Saving\n# ======================\nclass PeriodicPredictionSaverCallback(callbacks.Callback):\n    def __init__(self, test_spad_paths, save_freq, base_output_dir):\n        super().__init__()\n        self.test_spad_paths = test_spad_paths\n        self.save_freq = save_freq\n        self.base_output_dir = base_output_dir\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.save_freq == 0:\n            print(f\"\\nSaving predictions after epoch {epoch + 1}...\")\n            epoch_output_dir = os.path.join(self.base_output_dir, f\"epoch_{epoch+1:03d}\")\n            save_predictions_for_test_set_loop(\n                self.model,\n                self.test_spad_paths,\n                epoch_output_dir\n            )\n\n# ======================\n# Training Configuration\n# ======================\ndef main():\n    print(\"Preparing data file lists...\")\n    train_spad_paths, train_gt_paths = get_file_pairs(TRAIN_SPAD_DIR, TRAIN_GT_DIR)\n    val_spad_paths, val_gt_paths = get_file_pairs(VAL_SPAD_DIR, VAL_GT_DIR)\n    test_spad_paths = sorted([os.path.join(TEST_SPAD_DIR, f) for f in os.listdir(TEST_SPAD_DIR) if f.endswith('.png')])\n    print(f\"Training samples: {len(train_spad_paths)}\")\n    print(f\"Validation samples: {len(val_spad_paths)}\")\n    print(f\"Test samples: {len(test_spad_paths)}\")\n    if not train_spad_paths or not val_spad_paths or not test_spad_paths:\n        print(\"\\nError: One or more data directories are empty or paths are incorrect!\")\n        print(\"Please check DATA_DIR and subfolder names (TRAIN_SPAD_DIR, etc.)\")\n        return\n    train_gen = SPADGenerator(train_spad_paths, train_gt_paths, batch_size=BATCH_SIZE)\n    val_gen = SPADGenerator(val_spad_paths, val_gt_paths, batch_size=BATCH_SIZE)\n    print(\"Building model...\")\n    model = build_spadnet(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    print(\"Compiling model...\")\n    model.compile(\n        optimizer=tf.keras.optimizers.AdamW(LEARNING_RATE),\n        loss='mse',\n        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n    )\n    print(\"Setting up callbacks...\")\n    checkpoint_filepath = 'best_model.keras'\n    callbacks_list = [\n        callbacks.ModelCheckpoint(\n            checkpoint_filepath,\n            save_best_only=True,\n            monitor='val_rmse',\n            mode='min',\n            verbose=0\n        ),\n        PeriodicPredictionSaverCallback(\n            test_spad_paths=test_spad_paths,\n            save_freq=SAVE_PREDICTIONS_FREQ,\n            base_output_dir=BASE_PREDICTION_OUTPUT_DIR,\n        ),\n        callbacks.ReduceLROnPlateau(\n            monitor='val_rmse',\n            factor=0.5,\n            patience=3,\n            mode='min',\n            min_lr=1e-6\n        ),\n        callbacks.EarlyStopping(\n            monitor='val_rmse',\n            patience=10,\n            mode='min',\n            restore_best_weights=True\n        ),\n        TqdmCallback(verbose=1)\n    ]\n    print(\"Starting training...\")\n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=EPOCHS,\n        callbacks=callbacks_list,\n        verbose=0\n    )\n    print(\"Training finished.\")\n    if os.path.exists(checkpoint_filepath):\n        print(f\"Loading best model weights from {checkpoint_filepath}\")\n        model.load_weights(checkpoint_filepath)\n    else:\n        print(\"No checkpoint found. Using model state after last epoch.\")\n    print(\"\\nGenerating final predictions with the best model...\")\n    final_output_dir = os.path.join(BASE_PREDICTION_OUTPUT_DIR, \"final_best_model\")\n    save_predictions_for_test_set_loop(\n        model,\n        test_spad_paths,\n        final_output_dir\n    )\n    print(f\"Final predictions saved to {final_output_dir}\")\n    print(\"\\n--- Submission Step ---\")\n    print(\"1. Ensure you have the imgs2csv.py script provided by the challenge.\")\n    print(f\"2. The predicted depth maps are saved in subfolders of: {BASE_PREDICTION_OUTPUT_DIR}\")\n    print(\"3. Choose the folder you want to submit (e.g., 'final_best_model' for the best model, or 'epoch_XXX' for periodic saves).\")\n    print(\"4. Open a terminal/notebook cell and run the conversion script, pointing to the chosen folder:\")\n    print(f\"   python /kaggle/input/spad-depth-challenge/imgs2csv.py {final_output_dir} submission.csv\")\n    print(\"5. Submit the generated 'submission.csv' file to Kaggle.\")\n    print(\"\\nNote: Replace '/kaggle/input/spad-depth-challenge/imgs2csv.py' with the actual path to the imgs2csv.py script on Kaggle.\")\n    print(\"----------------------\")\n\nif __name__ == \"__main__\":\n    tf.keras.backend.set_floatx('float32')\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:28:46.939405Z","iopub.execute_input":"2025-04-29T11:28:46.939738Z","iopub.status.idle":"2025-04-29T11:45:43.749054Z","shell.execute_reply.started":"2025-04-29T11:28:46.939715Z","shell.execute_reply":"2025-04-29T11:45:43.748313Z"}},"outputs":[{"name":"stdout","text":"Preparing data file lists...\nTraining samples: 6686\nValidation samples: 836\nTest samples: 836\nBuilding model...\nCompiling model...\nSetting up callbacks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0epoch [00:00, ?epoch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1672682742b4afe838602a0ae82bdda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0batch [00:00, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745926137.658788     114 service.cc:148] XLA service 0x79f598003a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745926137.659697     114 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1745926138.561033     114 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1745926154.544686     114 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\nSaving predictions after epoch 5...\n","output_type":"stream"},{"name":"stderr","text":"\n\nSaving to epoch_005:   0%|          | 0/836 [00:00<?, ?it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   0%|          | 1/836 [00:01<20:44,  1.49s/it]\u001b[A\u001b[A\n\nSaving to epoch_005:   0%|          | 3/836 [00:01<06:06,  2.27it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   1%|          | 5/836 [00:01<03:25,  4.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   1%|          | 7/836 [00:01<02:20,  5.91it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   1%|          | 9/836 [00:02<01:48,  7.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   1%|â–         | 11/836 [00:02<01:29,  9.21it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   2%|â–         | 13/836 [00:02<01:19, 10.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   2%|â–         | 15/836 [00:02<01:10, 11.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   2%|â–         | 17/836 [00:02<01:04, 12.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   2%|â–         | 19/836 [00:02<01:00, 13.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   3%|â–Ž         | 21/836 [00:02<00:57, 14.19it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   3%|â–Ž         | 23/836 [00:02<00:57, 14.15it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   3%|â–Ž         | 25/836 [00:03<00:55, 14.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   3%|â–Ž         | 27/836 [00:03<00:53, 15.02it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   3%|â–Ž         | 29/836 [00:03<00:55, 14.56it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   4%|â–Ž         | 31/836 [00:03<00:54, 14.83it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   4%|â–         | 33/836 [00:03<00:54, 14.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   4%|â–         | 35/836 [00:03<00:53, 15.10it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   4%|â–         | 37/836 [00:03<00:52, 15.25it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   5%|â–         | 39/836 [00:03<00:52, 15.27it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   5%|â–         | 41/836 [00:04<00:53, 14.91it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   5%|â–Œ         | 43/836 [00:04<00:52, 15.14it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   5%|â–Œ         | 45/836 [00:04<00:53, 14.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   6%|â–Œ         | 47/836 [00:04<00:53, 14.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   6%|â–Œ         | 49/836 [00:04<00:52, 14.87it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   6%|â–Œ         | 51/836 [00:04<00:51, 15.15it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   6%|â–‹         | 53/836 [00:04<00:52, 15.02it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   7%|â–‹         | 55/836 [00:05<00:52, 14.94it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   7%|â–‹         | 57/836 [00:05<00:52, 14.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   7%|â–‹         | 59/836 [00:05<00:51, 15.10it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   7%|â–‹         | 61/836 [00:05<00:51, 15.00it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   8%|â–Š         | 63/836 [00:05<00:51, 14.97it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   8%|â–Š         | 65/836 [00:05<00:51, 15.06it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   8%|â–Š         | 67/836 [00:05<00:50, 15.29it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   8%|â–Š         | 69/836 [00:06<00:52, 14.58it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   8%|â–Š         | 71/836 [00:06<00:51, 14.87it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   9%|â–Š         | 73/836 [00:06<00:50, 15.08it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   9%|â–‰         | 75/836 [00:06<00:49, 15.24it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   9%|â–‰         | 77/836 [00:06<00:50, 15.11it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:   9%|â–‰         | 79/836 [00:06<00:49, 15.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  10%|â–‰         | 81/836 [00:06<00:49, 15.39it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  10%|â–‰         | 83/836 [00:06<00:48, 15.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  10%|â–ˆ         | 85/836 [00:07<00:48, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  10%|â–ˆ         | 87/836 [00:07<00:47, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  11%|â–ˆ         | 89/836 [00:07<00:47, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  11%|â–ˆ         | 91/836 [00:07<00:47, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  11%|â–ˆ         | 93/836 [00:07<00:47, 15.56it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  11%|â–ˆâ–        | 95/836 [00:07<00:47, 15.51it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  12%|â–ˆâ–        | 97/836 [00:07<00:47, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  12%|â–ˆâ–        | 99/836 [00:07<00:47, 15.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  12%|â–ˆâ–        | 101/836 [00:08<00:47, 15.56it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  12%|â–ˆâ–        | 103/836 [00:08<00:47, 15.56it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  13%|â–ˆâ–Ž        | 105/836 [00:08<00:49, 14.89it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  13%|â–ˆâ–Ž        | 107/836 [00:08<00:47, 15.20it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  13%|â–ˆâ–Ž        | 109/836 [00:08<00:47, 15.22it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  13%|â–ˆâ–Ž        | 111/836 [00:08<00:48, 15.04it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  14%|â–ˆâ–Ž        | 113/836 [00:08<00:47, 15.24it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  14%|â–ˆâ–        | 115/836 [00:09<00:47, 15.08it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  14%|â–ˆâ–        | 117/836 [00:09<00:47, 15.21it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  14%|â–ˆâ–        | 119/836 [00:09<00:48, 14.93it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  14%|â–ˆâ–        | 121/836 [00:09<00:47, 15.07it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  15%|â–ˆâ–        | 123/836 [00:09<00:47, 15.08it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  15%|â–ˆâ–        | 125/836 [00:09<00:46, 15.13it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  15%|â–ˆâ–Œ        | 127/836 [00:09<00:46, 15.35it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  15%|â–ˆâ–Œ        | 129/836 [00:09<00:46, 15.15it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  16%|â–ˆâ–Œ        | 131/836 [00:10<00:46, 15.01it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  16%|â–ˆâ–Œ        | 133/836 [00:10<00:46, 15.23it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  16%|â–ˆâ–Œ        | 135/836 [00:10<00:45, 15.42it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  16%|â–ˆâ–‹        | 137/836 [00:10<00:44, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  17%|â–ˆâ–‹        | 139/836 [00:10<00:44, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  17%|â–ˆâ–‹        | 141/836 [00:10<00:44, 15.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  17%|â–ˆâ–‹        | 143/836 [00:10<00:44, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  17%|â–ˆâ–‹        | 145/836 [00:10<00:44, 15.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  18%|â–ˆâ–Š        | 147/836 [00:11<00:44, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  18%|â–ˆâ–Š        | 149/836 [00:11<00:44, 15.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  18%|â–ˆâ–Š        | 151/836 [00:11<00:44, 15.39it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  18%|â–ˆâ–Š        | 153/836 [00:11<00:44, 15.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  19%|â–ˆâ–Š        | 155/836 [00:11<00:43, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  19%|â–ˆâ–‰        | 157/836 [00:11<00:44, 15.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  19%|â–ˆâ–‰        | 159/836 [00:11<00:43, 15.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  19%|â–ˆâ–‰        | 161/836 [00:11<00:43, 15.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  19%|â–ˆâ–‰        | 163/836 [00:12<00:43, 15.51it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  20%|â–ˆâ–‰        | 165/836 [00:12<00:43, 15.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  20%|â–ˆâ–‰        | 167/836 [00:12<00:42, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  20%|â–ˆâ–ˆ        | 169/836 [00:12<00:42, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  20%|â–ˆâ–ˆ        | 171/836 [00:12<00:43, 15.38it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  21%|â–ˆâ–ˆ        | 173/836 [00:12<00:42, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  21%|â–ˆâ–ˆ        | 175/836 [00:12<00:42, 15.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  21%|â–ˆâ–ˆ        | 177/836 [00:13<00:42, 15.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  21%|â–ˆâ–ˆâ–       | 179/836 [00:13<00:42, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  22%|â–ˆâ–ˆâ–       | 181/836 [00:13<00:41, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  22%|â–ˆâ–ˆâ–       | 183/836 [00:13<00:41, 15.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  22%|â–ˆâ–ˆâ–       | 185/836 [00:13<00:41, 15.74it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  22%|â–ˆâ–ˆâ–       | 187/836 [00:13<00:41, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  23%|â–ˆâ–ˆâ–Ž       | 189/836 [00:13<00:41, 15.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  23%|â–ˆâ–ˆâ–Ž       | 191/836 [00:13<00:41, 15.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  23%|â–ˆâ–ˆâ–Ž       | 193/836 [00:14<00:41, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  23%|â–ˆâ–ˆâ–Ž       | 195/836 [00:14<00:41, 15.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  24%|â–ˆâ–ˆâ–Ž       | 197/836 [00:14<00:41, 15.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  24%|â–ˆâ–ˆâ–       | 199/836 [00:14<00:41, 15.30it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  24%|â–ˆâ–ˆâ–       | 201/836 [00:14<00:41, 15.27it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  24%|â–ˆâ–ˆâ–       | 203/836 [00:14<00:41, 15.26it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  25%|â–ˆâ–ˆâ–       | 205/836 [00:14<00:41, 15.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  25%|â–ˆâ–ˆâ–       | 207/836 [00:14<00:40, 15.48it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  25%|â–ˆâ–ˆâ–Œ       | 209/836 [00:15<00:40, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  25%|â–ˆâ–ˆâ–Œ       | 211/836 [00:15<00:40, 15.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  25%|â–ˆâ–ˆâ–Œ       | 213/836 [00:15<00:40, 15.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  26%|â–ˆâ–ˆâ–Œ       | 215/836 [00:15<00:40, 15.24it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  26%|â–ˆâ–ˆâ–Œ       | 217/836 [00:15<00:40, 15.38it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  26%|â–ˆâ–ˆâ–Œ       | 219/836 [00:15<00:40, 15.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  26%|â–ˆâ–ˆâ–‹       | 221/836 [00:15<00:41, 14.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  27%|â–ˆâ–ˆâ–‹       | 223/836 [00:16<00:42, 14.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  27%|â–ˆâ–ˆâ–‹       | 225/836 [00:16<00:41, 14.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  27%|â–ˆâ–ˆâ–‹       | 227/836 [00:16<00:41, 14.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  27%|â–ˆâ–ˆâ–‹       | 229/836 [00:16<00:40, 14.89it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  28%|â–ˆâ–ˆâ–Š       | 231/836 [00:16<00:39, 15.18it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  28%|â–ˆâ–ˆâ–Š       | 233/836 [00:16<00:39, 15.42it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  28%|â–ˆâ–ˆâ–Š       | 235/836 [00:16<00:38, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  28%|â–ˆâ–ˆâ–Š       | 237/836 [00:16<00:38, 15.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  29%|â–ˆâ–ˆâ–Š       | 239/836 [00:17<00:38, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  29%|â–ˆâ–ˆâ–‰       | 241/836 [00:17<00:38, 15.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  29%|â–ˆâ–ˆâ–‰       | 243/836 [00:17<00:38, 15.48it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  29%|â–ˆâ–ˆâ–‰       | 245/836 [00:17<00:38, 15.48it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  30%|â–ˆâ–ˆâ–‰       | 247/836 [00:17<00:37, 15.58it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  30%|â–ˆâ–ˆâ–‰       | 249/836 [00:17<00:37, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  30%|â–ˆâ–ˆâ–ˆ       | 251/836 [00:17<00:37, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  30%|â–ˆâ–ˆâ–ˆ       | 253/836 [00:17<00:37, 15.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  31%|â–ˆâ–ˆâ–ˆ       | 255/836 [00:18<00:37, 15.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  31%|â–ˆâ–ˆâ–ˆ       | 257/836 [00:18<00:37, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  31%|â–ˆâ–ˆâ–ˆ       | 259/836 [00:18<00:36, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  31%|â–ˆâ–ˆâ–ˆ       | 261/836 [00:18<00:36, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  31%|â–ˆâ–ˆâ–ˆâ–      | 263/836 [00:18<00:36, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  32%|â–ˆâ–ˆâ–ˆâ–      | 265/836 [00:18<00:36, 15.44it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  32%|â–ˆâ–ˆâ–ˆâ–      | 267/836 [00:18<00:37, 15.35it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  32%|â–ˆâ–ˆâ–ˆâ–      | 269/836 [00:19<00:36, 15.35it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  32%|â–ˆâ–ˆâ–ˆâ–      | 271/836 [00:19<00:36, 15.48it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 273/836 [00:19<00:36, 15.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 275/836 [00:19<00:36, 15.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 277/836 [00:19<00:36, 15.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 279/836 [00:19<00:35, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 281/836 [00:19<00:35, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  34%|â–ˆâ–ˆâ–ˆâ–      | 283/836 [00:19<00:35, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  34%|â–ˆâ–ˆâ–ˆâ–      | 285/836 [00:20<00:35, 15.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  34%|â–ˆâ–ˆâ–ˆâ–      | 287/836 [00:20<00:35, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  35%|â–ˆâ–ˆâ–ˆâ–      | 289/836 [00:20<00:34, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  35%|â–ˆâ–ˆâ–ˆâ–      | 291/836 [00:20<00:34, 15.72it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 293/836 [00:20<00:34, 15.78it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 295/836 [00:20<00:34, 15.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 297/836 [00:20<00:34, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 299/836 [00:20<00:34, 15.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 301/836 [00:21<00:34, 15.35it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 303/836 [00:21<00:35, 15.17it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 305/836 [00:21<00:35, 15.03it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 307/836 [00:21<00:35, 14.77it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 309/836 [00:21<00:37, 14.13it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 311/836 [00:21<00:36, 14.28it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 313/836 [00:21<00:35, 14.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 315/836 [00:22<00:35, 14.78it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 317/836 [00:22<00:35, 14.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 319/836 [00:22<00:34, 14.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 321/836 [00:22<00:35, 14.58it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 323/836 [00:22<00:36, 14.18it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 325/836 [00:22<00:35, 14.42it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 327/836 [00:22<00:34, 14.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 329/836 [00:22<00:33, 15.23it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 331/836 [00:23<00:32, 15.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 333/836 [00:23<00:32, 15.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 335/836 [00:23<00:32, 15.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 337/836 [00:23<00:31, 15.81it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 339/836 [00:23<00:32, 15.23it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 341/836 [00:23<00:32, 15.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 343/836 [00:23<00:31, 15.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 345/836 [00:23<00:31, 15.65it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 347/836 [00:24<00:30, 15.81it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 349/836 [00:24<00:30, 15.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 351/836 [00:24<00:30, 15.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 353/836 [00:24<00:30, 15.86it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 355/836 [00:24<00:30, 15.88it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 357/836 [00:24<00:30, 15.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 359/836 [00:24<00:31, 15.38it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 361/836 [00:25<00:30, 15.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 363/836 [00:25<00:30, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 365/836 [00:25<00:30, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 367/836 [00:25<00:29, 15.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 369/836 [00:25<00:29, 15.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 371/836 [00:25<00:29, 15.80it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 373/836 [00:25<00:30, 15.25it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 375/836 [00:25<00:30, 14.94it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 377/836 [00:26<00:30, 14.98it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 379/836 [00:26<00:29, 15.25it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 381/836 [00:26<00:30, 14.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 383/836 [00:26<00:31, 14.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 385/836 [00:26<00:31, 14.10it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 387/836 [00:26<00:36, 12.24it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 389/836 [00:26<00:34, 12.96it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 391/836 [00:27<00:33, 13.35it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 393/836 [00:27<00:33, 13.29it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 395/836 [00:27<00:32, 13.38it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 397/836 [00:27<00:32, 13.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 399/836 [00:27<00:31, 13.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 401/836 [00:27<00:30, 14.14it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 403/836 [00:27<00:29, 14.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 405/836 [00:28<00:29, 14.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 407/836 [00:28<00:28, 15.15it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 409/836 [00:28<00:27, 15.32it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 411/836 [00:28<00:27, 15.21it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 413/836 [00:28<00:27, 15.22it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 415/836 [00:28<00:27, 15.27it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 417/836 [00:29<00:47,  8.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 419/836 [00:29<00:41,  9.97it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 421/836 [00:29<00:38, 10.91it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 423/836 [00:29<00:34, 11.99it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 425/836 [00:29<00:32, 12.77it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 427/836 [00:29<00:30, 13.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 429/836 [00:29<00:29, 13.91it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 431/836 [00:30<00:28, 14.29it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 433/836 [00:30<00:28, 14.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 435/836 [00:30<00:27, 14.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 437/836 [00:30<00:26, 14.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 439/836 [00:30<00:26, 15.15it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 441/836 [00:30<00:26, 15.06it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 443/836 [00:30<00:25, 15.31it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 445/836 [00:31<00:25, 15.15it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 447/836 [00:31<00:25, 15.19it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 449/836 [00:31<00:25, 15.21it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 451/836 [00:31<00:25, 15.13it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 453/836 [00:31<00:25, 15.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 455/836 [00:31<00:25, 15.00it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 457/836 [00:31<00:25, 14.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 459/836 [00:31<00:25, 14.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 461/836 [00:32<00:25, 14.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 463/836 [00:32<00:24, 15.02it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 465/836 [00:32<00:24, 15.06it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 467/836 [00:32<00:24, 15.23it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 469/836 [00:32<00:23, 15.32it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 471/836 [00:32<00:23, 15.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 473/836 [00:32<00:23, 15.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 475/836 [00:33<00:23, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 477/836 [00:33<00:22, 15.74it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 479/836 [00:33<00:22, 15.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 481/836 [00:33<00:22, 15.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 483/836 [00:33<00:22, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 485/836 [00:33<00:22, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 487/836 [00:33<00:22, 15.58it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 489/836 [00:33<00:22, 15.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 491/836 [00:34<00:22, 15.48it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 493/836 [00:34<00:22, 15.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 495/836 [00:34<00:21, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 497/836 [00:34<00:21, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 499/836 [00:34<00:21, 15.56it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 501/836 [00:34<00:21, 15.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 503/836 [00:34<00:21, 15.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 505/836 [00:34<00:21, 15.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 507/836 [00:35<00:21, 15.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 509/836 [00:35<00:21, 15.45it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 511/836 [00:35<00:21, 15.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 513/836 [00:35<00:20, 15.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 515/836 [00:35<00:20, 15.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 517/836 [00:35<00:20, 15.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 519/836 [00:35<00:20, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 521/836 [00:35<00:20, 15.01it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 523/836 [00:36<00:20, 15.24it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 525/836 [00:36<00:20, 15.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 527/836 [00:36<00:20, 15.26it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 529/836 [00:36<00:19, 15.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 531/836 [00:36<00:19, 15.58it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 533/836 [00:36<00:19, 15.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 535/836 [00:36<00:18, 15.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 537/836 [00:37<00:18, 15.83it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 539/836 [00:37<00:18, 15.89it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 541/836 [00:37<00:18, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 543/836 [00:37<00:18, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 545/836 [00:37<00:18, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 547/836 [00:37<00:18, 15.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 549/836 [00:37<00:18, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 551/836 [00:37<00:18, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 553/836 [00:38<00:17, 15.98it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 555/836 [00:38<00:17, 15.93it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 557/836 [00:38<00:17, 16.01it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 559/836 [00:38<00:17, 16.02it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 561/836 [00:38<00:17, 15.98it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 563/836 [00:38<00:17, 15.89it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 565/836 [00:38<00:17, 15.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 567/836 [00:38<00:17, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 569/836 [00:39<00:16, 15.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 571/836 [00:39<00:16, 15.87it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 573/836 [00:39<00:16, 15.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 575/836 [00:39<00:16, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 577/836 [00:39<00:16, 15.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 579/836 [00:39<00:16, 15.87it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 581/836 [00:39<00:16, 15.88it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 583/836 [00:39<00:15, 15.95it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 585/836 [00:40<00:15, 15.95it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 587/836 [00:40<00:15, 15.95it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 589/836 [00:40<00:15, 15.94it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 591/836 [00:40<00:15, 16.00it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 593/836 [00:40<00:15, 15.99it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 595/836 [00:40<00:15, 15.80it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 597/836 [00:40<00:15, 15.56it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 599/836 [00:40<00:15, 15.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 601/836 [00:41<00:15, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 603/836 [00:41<00:14, 15.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 605/836 [00:41<00:14, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 607/836 [00:41<00:14, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 609/836 [00:41<00:14, 15.65it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 611/836 [00:41<00:14, 15.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 613/836 [00:41<00:14, 15.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 615/836 [00:41<00:14, 15.65it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 617/836 [00:42<00:13, 15.77it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 619/836 [00:42<00:13, 15.83it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 621/836 [00:42<00:13, 15.74it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 623/836 [00:42<00:13, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 625/836 [00:42<00:13, 15.77it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 627/836 [00:42<00:13, 15.77it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 629/836 [00:42<00:13, 15.89it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 631/836 [00:42<00:12, 15.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 633/836 [00:43<00:12, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 635/836 [00:43<00:12, 15.88it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 637/836 [00:43<00:12, 15.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 639/836 [00:43<00:12, 15.91it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 641/836 [00:43<00:12, 15.99it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 643/836 [00:43<00:12, 16.01it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 645/836 [00:43<00:12, 15.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 647/836 [00:43<00:11, 15.96it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 649/836 [00:44<00:11, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 651/836 [00:44<00:11, 15.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 653/836 [00:44<00:11, 15.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 655/836 [00:44<00:11, 15.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 657/836 [00:44<00:11, 15.45it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 659/836 [00:44<00:11, 15.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 661/836 [00:44<00:11, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 663/836 [00:44<00:10, 15.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 665/836 [00:45<00:10, 15.91it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 667/836 [00:45<00:10, 15.99it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 669/836 [00:45<00:10, 15.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 671/836 [00:45<00:10, 15.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 673/836 [00:45<00:10, 15.96it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 675/836 [00:45<00:10, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 677/836 [00:45<00:10, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 679/836 [00:46<00:10, 15.14it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 681/836 [00:46<00:10, 15.19it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 683/836 [00:46<00:09, 15.31it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 685/836 [00:46<00:09, 15.51it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 687/836 [00:46<00:09, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 689/836 [00:46<00:09, 15.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 691/836 [00:46<00:09, 15.74it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 693/836 [00:46<00:09, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 695/836 [00:47<00:08, 15.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 697/836 [00:47<00:08, 15.86it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 699/836 [00:47<00:08, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 701/836 [00:47<00:08, 15.78it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 703/836 [00:47<00:08, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 705/836 [00:47<00:08, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 707/836 [00:47<00:08, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 709/836 [00:47<00:08, 15.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 711/836 [00:48<00:08, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 713/836 [00:48<00:07, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 715/836 [00:48<00:07, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 717/836 [00:48<00:07, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 719/836 [00:48<00:07, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 721/836 [00:48<00:07, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 723/836 [00:48<00:07, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 725/836 [00:48<00:07, 15.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 727/836 [00:49<00:06, 15.83it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 729/836 [00:49<00:06, 15.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 731/836 [00:49<00:06, 15.74it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 733/836 [00:49<00:06, 15.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 735/836 [00:49<00:06, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 737/836 [00:49<00:06, 15.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 739/836 [00:49<00:06, 15.97it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 741/836 [00:49<00:05, 15.87it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 743/836 [00:50<00:05, 15.88it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 745/836 [00:50<00:05, 15.81it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 747/836 [00:50<00:05, 15.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 749/836 [00:50<00:05, 15.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 751/836 [00:50<00:05, 15.83it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 753/836 [00:50<00:05, 15.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 755/836 [00:50<00:05, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 757/836 [00:50<00:05, 14.95it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 759/836 [00:51<00:05, 15.21it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 761/836 [00:51<00:04, 15.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 763/836 [00:51<00:04, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 765/836 [00:51<00:04, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 767/836 [00:51<00:04, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 769/836 [00:51<00:04, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 771/836 [00:51<00:04, 15.72it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 773/836 [00:52<00:03, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 775/836 [00:52<00:03, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 777/836 [00:52<00:03, 15.27it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 779/836 [00:52<00:03, 15.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 781/836 [00:52<00:03, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 783/836 [00:52<00:03, 15.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 785/836 [00:52<00:03, 15.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 787/836 [00:52<00:03, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 789/836 [00:53<00:03, 14.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 791/836 [00:53<00:02, 15.23it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 793/836 [00:53<00:02, 15.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 795/836 [00:53<00:02, 15.50it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 797/836 [00:53<00:02, 15.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 799/836 [00:53<00:02, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 801/836 [00:53<00:02, 15.72it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 803/836 [00:53<00:02, 15.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 805/836 [00:54<00:01, 15.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 807/836 [00:54<00:01, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 809/836 [00:54<00:01, 15.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 811/836 [00:54<00:01, 15.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 813/836 [00:54<00:01, 15.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 815/836 [00:54<00:01, 15.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 817/836 [00:54<00:01, 15.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 819/836 [00:54<00:01, 15.77it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 821/836 [00:55<00:00, 15.86it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 823/836 [00:55<00:00, 15.99it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 825/836 [00:55<00:00, 15.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 827/836 [00:55<00:00, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 829/836 [00:55<00:00, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_005:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 831/836 [00:55<00:00, 15.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_005: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 833/836 [00:55<00:00, 15.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_005: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 836/836 [00:56<00:00, 14.91it/s]\u001b[A\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"\nSaving predictions after epoch 10...\n","output_type":"stream"},{"name":"stderr","text":"\n\nSaving to epoch_010:   0%|          | 0/836 [00:00<?, ?it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   0%|          | 2/836 [00:00<00:59, 14.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   0%|          | 4/836 [00:00<00:53, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   1%|          | 6/836 [00:00<00:51, 16.27it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   1%|          | 8/836 [00:00<00:51, 16.14it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   1%|          | 10/836 [00:00<00:50, 16.34it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   1%|â–         | 12/836 [00:00<00:49, 16.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   2%|â–         | 14/836 [00:00<00:49, 16.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   2%|â–         | 16/836 [00:00<00:49, 16.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   2%|â–         | 18/836 [00:01<00:48, 16.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   2%|â–         | 20/836 [00:01<00:48, 16.87it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   3%|â–Ž         | 22/836 [00:01<00:47, 16.97it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   3%|â–Ž         | 24/836 [00:01<00:47, 17.01it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   3%|â–Ž         | 26/836 [00:01<00:52, 15.50it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   3%|â–Ž         | 28/836 [00:01<00:53, 15.03it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   4%|â–Ž         | 30/836 [00:01<00:53, 14.96it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   4%|â–         | 32/836 [00:02<00:53, 14.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   4%|â–         | 34/836 [00:02<00:51, 15.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   4%|â–         | 36/836 [00:02<00:51, 15.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   5%|â–         | 38/836 [00:02<00:52, 15.26it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   5%|â–         | 40/836 [00:02<00:51, 15.42it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   5%|â–Œ         | 42/836 [00:02<00:50, 15.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   5%|â–Œ         | 44/836 [00:02<00:49, 15.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   6%|â–Œ         | 46/836 [00:02<00:48, 16.17it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   6%|â–Œ         | 48/836 [00:03<00:48, 16.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   6%|â–Œ         | 50/836 [00:03<00:47, 16.39it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   6%|â–Œ         | 52/836 [00:03<00:47, 16.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   6%|â–‹         | 54/836 [00:03<00:46, 16.65it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   7%|â–‹         | 56/836 [00:03<00:46, 16.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   7%|â–‹         | 58/836 [00:03<00:46, 16.80it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   7%|â–‹         | 60/836 [00:03<00:46, 16.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   7%|â–‹         | 62/836 [00:03<00:46, 16.65it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   8%|â–Š         | 64/836 [00:03<00:46, 16.78it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   8%|â–Š         | 66/836 [00:04<00:45, 16.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   8%|â–Š         | 68/836 [00:04<00:45, 16.93it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   8%|â–Š         | 70/836 [00:04<00:45, 17.00it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   9%|â–Š         | 72/836 [00:04<00:44, 17.01it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   9%|â–‰         | 74/836 [00:04<00:45, 16.88it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   9%|â–‰         | 76/836 [00:04<00:45, 16.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:   9%|â–‰         | 78/836 [00:04<00:46, 16.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  10%|â–‰         | 80/836 [00:04<00:45, 16.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  10%|â–‰         | 82/836 [00:05<00:45, 16.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  10%|â–ˆ         | 84/836 [00:05<00:45, 16.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  10%|â–ˆ         | 86/836 [00:05<00:48, 15.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  11%|â–ˆ         | 88/836 [00:05<00:47, 15.89it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  11%|â–ˆ         | 90/836 [00:05<00:46, 16.15it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  11%|â–ˆ         | 92/836 [00:05<00:45, 16.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  11%|â–ˆ         | 94/836 [00:05<00:44, 16.51it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  11%|â–ˆâ–        | 96/836 [00:05<00:44, 16.72it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  12%|â–ˆâ–        | 98/836 [00:06<00:44, 16.50it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  12%|â–ˆâ–        | 100/836 [00:06<00:44, 16.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  12%|â–ˆâ–        | 102/836 [00:06<00:43, 16.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  12%|â–ˆâ–        | 104/836 [00:06<00:43, 16.77it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  13%|â–ˆâ–Ž        | 106/836 [00:06<00:43, 16.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  13%|â–ˆâ–Ž        | 108/836 [00:06<00:42, 16.94it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  13%|â–ˆâ–Ž        | 110/836 [00:06<00:43, 16.86it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  13%|â–ˆâ–Ž        | 112/836 [00:06<00:42, 16.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  14%|â–ˆâ–Ž        | 114/836 [00:06<00:42, 16.97it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  14%|â–ˆâ–        | 116/836 [00:07<00:42, 16.98it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  14%|â–ˆâ–        | 118/836 [00:07<00:42, 17.06it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  14%|â–ˆâ–        | 120/836 [00:07<00:42, 17.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  15%|â–ˆâ–        | 122/836 [00:07<00:42, 16.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  15%|â–ˆâ–        | 124/836 [00:07<00:43, 16.24it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  15%|â–ˆâ–Œ        | 126/836 [00:07<00:43, 16.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  15%|â–ˆâ–Œ        | 128/836 [00:07<00:42, 16.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  16%|â–ˆâ–Œ        | 130/836 [00:07<00:42, 16.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  16%|â–ˆâ–Œ        | 132/836 [00:08<00:42, 16.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  16%|â–ˆâ–Œ        | 134/836 [00:08<00:42, 16.58it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  16%|â–ˆâ–‹        | 136/836 [00:08<00:42, 16.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  17%|â–ˆâ–‹        | 138/836 [00:08<00:41, 16.72it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  17%|â–ˆâ–‹        | 140/836 [00:08<00:41, 16.81it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  17%|â–ˆâ–‹        | 142/836 [00:08<00:41, 16.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  17%|â–ˆâ–‹        | 144/836 [00:08<00:41, 16.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  17%|â–ˆâ–‹        | 146/836 [00:08<00:41, 16.65it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  18%|â–ˆâ–Š        | 148/836 [00:09<00:41, 16.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  18%|â–ˆâ–Š        | 150/836 [00:09<00:40, 16.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  18%|â–ˆâ–Š        | 152/836 [00:09<00:40, 16.78it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  18%|â–ˆâ–Š        | 154/836 [00:09<00:40, 16.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  19%|â–ˆâ–Š        | 156/836 [00:09<00:40, 16.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  19%|â–ˆâ–‰        | 158/836 [00:09<00:40, 16.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  19%|â–ˆâ–‰        | 160/836 [00:09<00:41, 16.35it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  19%|â–ˆâ–‰        | 162/836 [00:09<00:41, 16.32it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  20%|â–ˆâ–‰        | 164/836 [00:09<00:41, 16.39it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  20%|â–ˆâ–‰        | 166/836 [00:10<00:40, 16.38it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  20%|â–ˆâ–ˆ        | 168/836 [00:10<00:41, 16.13it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  20%|â–ˆâ–ˆ        | 170/836 [00:10<00:41, 16.12it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  21%|â–ˆâ–ˆ        | 172/836 [00:10<00:41, 16.15it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  21%|â–ˆâ–ˆ        | 174/836 [00:10<00:41, 16.04it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  21%|â–ˆâ–ˆ        | 176/836 [00:10<00:40, 16.12it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  21%|â–ˆâ–ˆâ–       | 178/836 [00:10<00:41, 15.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  22%|â–ˆâ–ˆâ–       | 180/836 [00:10<00:40, 16.16it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  22%|â–ˆâ–ˆâ–       | 182/836 [00:11<00:40, 16.19it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  22%|â–ˆâ–ˆâ–       | 184/836 [00:11<00:39, 16.32it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  22%|â–ˆâ–ˆâ–       | 186/836 [00:11<00:39, 16.32it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  22%|â–ˆâ–ˆâ–       | 188/836 [00:11<00:39, 16.37it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  23%|â–ˆâ–ˆâ–Ž       | 190/836 [00:11<00:39, 16.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  23%|â–ˆâ–ˆâ–Ž       | 192/836 [00:11<00:39, 16.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  23%|â–ˆâ–ˆâ–Ž       | 194/836 [00:11<00:39, 16.25it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  23%|â–ˆâ–ˆâ–Ž       | 196/836 [00:11<00:38, 16.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  24%|â–ˆâ–ˆâ–Ž       | 198/836 [00:12<00:38, 16.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  24%|â–ˆâ–ˆâ–       | 200/836 [00:12<00:38, 16.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  24%|â–ˆâ–ˆâ–       | 202/836 [00:12<00:38, 16.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  24%|â–ˆâ–ˆâ–       | 204/836 [00:12<00:38, 16.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  25%|â–ˆâ–ˆâ–       | 206/836 [00:12<00:38, 16.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  25%|â–ˆâ–ˆâ–       | 208/836 [00:12<00:38, 16.28it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  25%|â–ˆâ–ˆâ–Œ       | 210/836 [00:12<00:38, 16.44it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  25%|â–ˆâ–ˆâ–Œ       | 212/836 [00:12<00:37, 16.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  26%|â–ˆâ–ˆâ–Œ       | 214/836 [00:13<00:38, 16.34it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  26%|â–ˆâ–ˆâ–Œ       | 216/836 [00:13<00:38, 16.27it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  26%|â–ˆâ–ˆâ–Œ       | 218/836 [00:13<00:38, 16.03it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  26%|â–ˆâ–ˆâ–‹       | 220/836 [00:13<00:38, 15.97it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  27%|â–ˆâ–ˆâ–‹       | 222/836 [00:13<00:38, 15.86it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  27%|â–ˆâ–ˆâ–‹       | 224/836 [00:13<00:38, 15.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  27%|â–ˆâ–ˆâ–‹       | 226/836 [00:13<00:39, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  27%|â–ˆâ–ˆâ–‹       | 228/836 [00:13<00:38, 15.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  28%|â–ˆâ–ˆâ–Š       | 230/836 [00:14<00:38, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  28%|â–ˆâ–ˆâ–Š       | 232/836 [00:14<00:38, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  28%|â–ˆâ–ˆâ–Š       | 234/836 [00:14<00:38, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  28%|â–ˆâ–ˆâ–Š       | 236/836 [00:14<00:38, 15.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  28%|â–ˆâ–ˆâ–Š       | 238/836 [00:14<00:37, 15.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  29%|â–ˆâ–ˆâ–Š       | 240/836 [00:14<00:38, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  29%|â–ˆâ–ˆâ–‰       | 242/836 [00:14<00:38, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  29%|â–ˆâ–ˆâ–‰       | 244/836 [00:14<00:37, 15.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  29%|â–ˆâ–ˆâ–‰       | 246/836 [00:15<00:37, 15.74it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  30%|â–ˆâ–ˆâ–‰       | 248/836 [00:15<00:37, 15.65it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  30%|â–ˆâ–ˆâ–‰       | 250/836 [00:15<00:40, 14.45it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  30%|â–ˆâ–ˆâ–ˆ       | 252/836 [00:15<00:40, 14.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  30%|â–ˆâ–ˆâ–ˆ       | 254/836 [00:15<00:39, 14.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  31%|â–ˆâ–ˆâ–ˆ       | 256/836 [00:15<00:38, 15.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  31%|â–ˆâ–ˆâ–ˆ       | 258/836 [00:15<00:37, 15.26it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  31%|â–ˆâ–ˆâ–ˆ       | 260/836 [00:16<00:37, 15.39it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  31%|â–ˆâ–ˆâ–ˆâ–      | 262/836 [00:16<00:36, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  32%|â–ˆâ–ˆâ–ˆâ–      | 264/836 [00:16<00:36, 15.72it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  32%|â–ˆâ–ˆâ–ˆâ–      | 266/836 [00:16<00:36, 15.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  32%|â–ˆâ–ˆâ–ˆâ–      | 268/836 [00:16<00:36, 15.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  32%|â–ˆâ–ˆâ–ˆâ–      | 270/836 [00:16<00:36, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 272/836 [00:16<00:35, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 274/836 [00:16<00:35, 15.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 276/836 [00:17<00:35, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 278/836 [00:17<00:35, 15.74it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 280/836 [00:17<00:35, 15.55it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 282/836 [00:17<00:35, 15.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  34%|â–ˆâ–ˆâ–ˆâ–      | 284/836 [00:17<00:35, 15.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  34%|â–ˆâ–ˆâ–ˆâ–      | 286/836 [00:17<00:35, 15.58it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  34%|â–ˆâ–ˆâ–ˆâ–      | 288/836 [00:17<00:34, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  35%|â–ˆâ–ˆâ–ˆâ–      | 290/836 [00:17<00:35, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  35%|â–ˆâ–ˆâ–ˆâ–      | 292/836 [00:18<00:34, 15.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 294/836 [00:18<00:34, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 296/836 [00:18<00:34, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 298/836 [00:18<00:34, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 300/836 [00:18<00:33, 15.77it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 302/836 [00:18<00:34, 15.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 304/836 [00:18<00:34, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 306/836 [00:18<00:33, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 308/836 [00:19<00:33, 15.56it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 310/836 [00:19<00:33, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 312/836 [00:19<00:33, 15.83it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 314/836 [00:19<00:33, 15.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 316/836 [00:19<00:33, 15.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 318/836 [00:19<00:33, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 320/836 [00:19<00:33, 15.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 322/836 [00:19<00:32, 15.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 324/836 [00:20<00:32, 15.81it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 326/836 [00:20<00:32, 15.87it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 328/836 [00:20<00:31, 15.91it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 330/836 [00:20<00:31, 16.04it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 332/836 [00:20<00:31, 16.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 334/836 [00:20<00:31, 16.02it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 336/836 [00:20<00:31, 15.99it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 338/836 [00:20<00:31, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 340/836 [00:21<00:31, 15.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 342/836 [00:21<00:31, 15.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 344/836 [00:21<00:31, 15.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 346/836 [00:21<00:31, 15.80it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 348/836 [00:21<00:30, 16.00it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 350/836 [00:21<00:30, 15.88it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 352/836 [00:21<00:30, 15.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 354/836 [00:21<00:30, 15.91it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 356/836 [00:22<00:30, 15.96it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 358/836 [00:22<00:29, 16.00it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 360/836 [00:22<00:29, 16.08it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 362/836 [00:22<00:29, 15.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 364/836 [00:22<00:29, 15.88it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 366/836 [00:22<00:29, 15.86it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 368/836 [00:22<00:29, 15.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 370/836 [00:22<00:29, 15.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 372/836 [00:23<00:29, 15.89it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 374/836 [00:23<00:29, 15.80it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 376/836 [00:23<00:29, 15.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 378/836 [00:23<00:28, 15.80it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 380/836 [00:23<00:28, 15.78it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 382/836 [00:23<00:29, 15.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 384/836 [00:23<00:29, 15.30it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 386/836 [00:24<00:29, 15.34it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 388/836 [00:24<00:29, 15.13it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 390/836 [00:24<00:29, 15.30it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 392/836 [00:24<00:28, 15.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 394/836 [00:24<00:28, 15.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 396/836 [00:24<00:28, 15.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 398/836 [00:24<00:27, 15.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 400/836 [00:24<00:27, 15.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 402/836 [00:25<00:27, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 404/836 [00:25<00:27, 15.89it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 406/836 [00:25<00:28, 15.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 408/836 [00:25<00:27, 15.37it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 410/836 [00:25<00:27, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 412/836 [00:25<00:27, 15.56it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 414/836 [00:25<00:26, 15.75it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 416/836 [00:25<00:26, 15.97it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 418/836 [00:26<00:25, 16.18it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 420/836 [00:26<00:25, 16.08it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 422/836 [00:26<00:25, 16.17it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 424/836 [00:26<00:25, 16.13it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 426/836 [00:26<00:25, 16.15it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 428/836 [00:26<00:25, 16.14it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 430/836 [00:26<00:25, 16.04it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 432/836 [00:26<00:25, 16.03it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 434/836 [00:27<00:25, 15.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 436/836 [00:27<00:25, 15.87it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 438/836 [00:27<00:25, 15.87it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 440/836 [00:27<00:25, 15.83it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 442/836 [00:27<00:25, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 444/836 [00:27<00:24, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 446/836 [00:27<00:24, 15.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 448/836 [00:27<00:24, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 450/836 [00:28<00:24, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 452/836 [00:28<00:24, 15.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 454/836 [00:28<00:24, 15.69it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 456/836 [00:28<00:24, 15.72it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 458/836 [00:28<00:24, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 460/836 [00:28<00:24, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 462/836 [00:28<00:24, 15.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 464/836 [00:28<00:23, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 466/836 [00:29<00:23, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 468/836 [00:29<00:23, 15.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 470/836 [00:29<00:23, 15.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 472/836 [00:29<00:23, 15.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 474/836 [00:29<00:23, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 476/836 [00:29<00:23, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 478/836 [00:29<00:22, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 480/836 [00:30<00:22, 15.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 482/836 [00:30<00:22, 15.60it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 484/836 [00:30<00:22, 15.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 486/836 [00:30<00:22, 15.80it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 488/836 [00:30<00:21, 16.07it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 490/836 [00:30<00:21, 15.92it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 492/836 [00:30<00:21, 15.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 494/836 [00:30<00:21, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 496/836 [00:31<00:21, 15.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 498/836 [00:31<00:21, 15.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 500/836 [00:31<00:21, 15.85it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 502/836 [00:31<00:20, 16.19it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 504/836 [00:31<00:20, 16.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 506/836 [00:31<00:20, 16.29it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 508/836 [00:31<00:20, 16.25it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 510/836 [00:31<00:19, 16.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 512/836 [00:31<00:19, 16.50it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 514/836 [00:32<00:19, 16.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 516/836 [00:32<00:19, 16.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 518/836 [00:32<00:19, 16.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 520/836 [00:32<00:18, 16.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 522/836 [00:32<00:18, 16.67it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 524/836 [00:32<00:18, 16.56it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 526/836 [00:32<00:20, 15.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 528/836 [00:33<00:21, 14.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 530/836 [00:33<00:21, 14.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 532/836 [00:33<00:21, 14.04it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 534/836 [00:33<00:20, 14.43it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 536/836 [00:33<00:20, 14.94it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 538/836 [00:33<00:19, 15.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 540/836 [00:33<00:18, 15.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 542/836 [00:33<00:18, 15.95it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 544/836 [00:34<00:18, 16.11it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 546/836 [00:34<00:17, 16.33it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 548/836 [00:34<00:17, 16.48it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 550/836 [00:34<00:17, 16.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 552/836 [00:34<00:17, 16.06it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 554/836 [00:34<00:17, 15.91it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 556/836 [00:34<00:17, 16.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 558/836 [00:34<00:17, 16.00it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 560/836 [00:35<00:17, 16.10it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 562/836 [00:35<00:17, 15.79it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 564/836 [00:35<00:18, 14.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 566/836 [00:35<00:18, 14.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 568/836 [00:35<00:17, 15.10it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 570/836 [00:35<00:17, 15.44it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 572/836 [00:35<00:16, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 574/836 [00:35<00:16, 15.99it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 576/836 [00:36<00:16, 16.18it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 578/836 [00:36<00:15, 16.25it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 580/836 [00:36<00:15, 16.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 582/836 [00:36<00:15, 16.51it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 584/836 [00:36<00:15, 16.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 586/836 [00:36<00:15, 16.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 588/836 [00:36<00:14, 16.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 590/836 [00:36<00:14, 16.51it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 592/836 [00:37<00:14, 16.57it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 594/836 [00:37<00:14, 16.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 596/836 [00:37<00:14, 16.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 598/836 [00:37<00:14, 16.80it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/836 [00:37<00:14, 16.73it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 602/836 [00:37<00:14, 16.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 604/836 [00:37<00:13, 16.70it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 606/836 [00:37<00:13, 16.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 608/836 [00:37<00:13, 16.65it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 610/836 [00:38<00:13, 16.58it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 612/836 [00:38<00:13, 16.65it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 614/836 [00:38<00:13, 16.59it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 616/836 [00:38<00:13, 16.63it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 618/836 [00:38<00:13, 16.64it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 620/836 [00:38<00:12, 16.62it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 622/836 [00:38<00:12, 16.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 624/836 [00:38<00:12, 16.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 626/836 [00:39<00:12, 16.48it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 628/836 [00:39<00:12, 16.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 630/836 [00:39<00:12, 16.44it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 632/836 [00:39<00:12, 16.45it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 634/836 [00:39<00:12, 16.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 636/836 [00:39<00:12, 16.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 638/836 [00:39<00:12, 16.44it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 640/836 [00:39<00:11, 16.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 642/836 [00:40<00:11, 16.52it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 644/836 [00:40<00:11, 16.51it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 646/836 [00:40<00:11, 16.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 648/836 [00:40<00:11, 16.42it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 650/836 [00:40<00:11, 16.29it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 652/836 [00:40<00:11, 16.10it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 654/836 [00:40<00:11, 16.18it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 656/836 [00:40<00:11, 16.23it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 658/836 [00:41<00:10, 16.39it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 660/836 [00:41<00:10, 16.48it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 662/836 [00:41<00:10, 16.14it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 664/836 [00:41<00:10, 16.05it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 666/836 [00:41<00:10, 16.09it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 668/836 [00:41<00:10, 16.07it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 670/836 [00:41<00:10, 16.11it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 672/836 [00:41<00:10, 16.19it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 674/836 [00:42<00:09, 16.25it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 676/836 [00:42<00:09, 16.31it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 678/836 [00:42<00:09, 16.29it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 680/836 [00:42<00:09, 16.34it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 682/836 [00:42<00:09, 16.41it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 684/836 [00:42<00:09, 16.51it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 686/836 [00:42<00:09, 16.33it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 688/836 [00:42<00:09, 16.37it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 690/836 [00:43<00:08, 16.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 692/836 [00:43<00:08, 16.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 694/836 [00:43<00:08, 16.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 696/836 [00:43<00:08, 16.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 698/836 [00:43<00:08, 16.47it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 700/836 [00:43<00:08, 16.44it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 702/836 [00:43<00:08, 16.34it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 704/836 [00:43<00:08, 16.37it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 706/836 [00:43<00:07, 16.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 708/836 [00:44<00:07, 16.54it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 710/836 [00:44<00:07, 16.07it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 712/836 [00:44<00:07, 16.22it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 714/836 [00:44<00:07, 16.34it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 716/836 [00:44<00:07, 16.34it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 718/836 [00:44<00:07, 16.29it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 720/836 [00:44<00:07, 16.42it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 722/836 [00:44<00:06, 16.32it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 724/836 [00:45<00:06, 16.26it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 726/836 [00:45<00:06, 15.82it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 728/836 [00:45<00:07, 15.28it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 730/836 [00:45<00:06, 15.53it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 732/836 [00:45<00:06, 15.68it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 734/836 [00:45<00:06, 15.71it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 736/836 [00:45<00:06, 15.93it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 738/836 [00:45<00:06, 15.98it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 740/836 [00:46<00:06, 15.90it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 742/836 [00:46<00:05, 16.10it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 744/836 [00:46<00:05, 16.31it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 746/836 [00:46<00:05, 16.21it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 748/836 [00:46<00:05, 16.18it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 750/836 [00:46<00:05, 16.22it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 752/836 [00:46<00:05, 16.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 754/836 [00:46<00:05, 16.34it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 756/836 [00:47<00:04, 16.46it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 758/836 [00:47<00:04, 16.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 760/836 [00:47<00:04, 16.44it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 762/836 [00:47<00:04, 16.38it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 764/836 [00:47<00:04, 16.40it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 766/836 [00:47<00:04, 16.50it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 768/836 [00:47<00:04, 16.24it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 770/836 [00:47<00:04, 15.78it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 772/836 [00:48<00:04, 15.89it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 774/836 [00:48<00:03, 16.08it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 776/836 [00:48<00:03, 16.20it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 778/836 [00:48<00:03, 16.23it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 780/836 [00:48<00:03, 16.35it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 782/836 [00:48<00:03, 16.30it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 784/836 [00:48<00:03, 16.28it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 786/836 [00:48<00:03, 16.23it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 788/836 [00:49<00:02, 16.37it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 790/836 [00:49<00:02, 16.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 792/836 [00:49<00:02, 16.50it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 794/836 [00:49<00:02, 16.31it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 796/836 [00:49<00:02, 16.33it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 798/836 [00:49<00:02, 16.03it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 800/836 [00:49<00:02, 15.66it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 802/836 [00:49<00:02, 15.84it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 804/836 [00:50<00:01, 16.12it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 806/836 [00:50<00:01, 16.09it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 808/836 [00:50<00:01, 16.04it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 810/836 [00:50<00:01, 16.14it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 812/836 [00:50<00:01, 16.30it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 814/836 [00:50<00:01, 16.27it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 816/836 [00:50<00:01, 16.28it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 818/836 [00:50<00:01, 16.23it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 820/836 [00:51<00:00, 16.36it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 822/836 [00:51<00:00, 16.49it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 824/836 [00:51<00:00, 16.61it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 826/836 [00:51<00:00, 16.76it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 828/836 [00:51<00:00, 16.88it/s]\u001b[A\u001b[A\n\nSaving to epoch_010:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 830/836 [00:51<00:00, 16.78it/s]\u001b[A\u001b[A\n\nSaving to epoch_010: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 832/836 [00:51<00:00, 16.80it/s]\u001b[A\u001b[A\n\nSaving to epoch_010: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 834/836 [00:51<00:00, 16.86it/s]\u001b[A\u001b[A\n\nSaving to epoch_010: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 836/836 [00:51<00:00, 16.08it/s]\u001b[A\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Training finished.\nLoading best model weights from best_model.keras\n\nGenerating final predictions with the best model...\n","output_type":"stream"},{"name":"stderr","text":"Saving to final_best_model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 836/836 [00:50<00:00, 16.65it/s]","output_type":"stream"},{"name":"stdout","text":"Final predictions saved to /kaggle/working/predictions_png/final_best_model\n\n--- Submission Step ---\n1. Ensure you have the imgs2csv.py script provided by the challenge.\n2. The predicted depth maps are saved in subfolders of: /kaggle/working/predictions_png\n3. Choose the folder you want to submit (e.g., 'final_best_model' for the best model, or 'epoch_XXX' for periodic saves).\n4. Open a terminal/notebook cell and run the conversion script, pointing to the chosen folder:\n   python /kaggle/input/spad-depth-challenge/imgs2csv.py /kaggle/working/predictions_png/final_best_model submission.csv\n5. Submit the generated 'submission.csv' file to Kaggle.\n\nNote: Replace '/kaggle/input/spad-depth-challenge/imgs2csv.py' with the actual path to the imgs2csv.py script on Kaggle.\n----------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nimport numpy as np\n\ndef images_to_csv_with_metadata(image_folder, output_csv):\n    # Initialize an empty list to store image data and metadata\n    data = []\n\n    # Loop through all images in the folder\n    for idx, filename in enumerate(sorted(os.listdir(image_folder))):\n        if filename.endswith(\".png\"):\n            filepath = os.path.join(image_folder, filename)\n            # Read the image\n            image = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n            image = cv2.resize(image, (128, 128))\n            image = image / 255.\n            image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-6)\n            image = np.uint8(image * 255.)\n            # Flatten the image into a 1D array\n            image_flat = image.flatten()\n            # Add ID, ImageID (filename), and pixel values\n            row = [idx, filename] + image_flat.tolist()\n            data.append(row)\n    \n    # Create a DataFrame\n    num_columns = len(data[0]) - 2 if data else 0\n    column_names = [\"id\", \"ImageID\"] + [indx for indx in range(num_columns)]\n    df = pd.DataFrame(data, columns=column_names)\n\n    # Save to CSV\n    df.to_csv(output_csv, index=False)\n\n# Paths for prediction and ground truth images\npredictions_folder = \"/kaggle/working/predictions_png/epoch_010\"\n\n# Output CSV paths\npredictions_csv = \"/kaggle/working/predictions_10.csv\"\n\n# Convert prediction images to CSV\nimages_to_csv_with_metadata(predictions_folder, predictions_csv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:56:17.707813Z","iopub.execute_input":"2025-04-29T11:56:17.708350Z","iopub.status.idle":"2025-04-29T11:56:26.198848Z","shell.execute_reply.started":"2025-04-29T11:56:17.708328Z","shell.execute_reply":"2025-04-29T11:56:26.198050Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}